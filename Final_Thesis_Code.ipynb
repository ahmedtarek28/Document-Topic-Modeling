{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JIBzlEt01UHG",
        "axVQb9XB2u4V",
        "628lAYuZ5Bet",
        "DuJWTQdZ39pZ",
        "ej1qJCAo4xOn",
        "rI_n4O_67i3p",
        "2Sgj_DK18fIW",
        "43I7SCtV-HSm",
        "y22m7Km-Es2H",
        "DVD3G9hIFWrr",
        "wOsYGGEoPJhH"
      ],
      "mount_file_id": "1PSTsPKIyLQs1B-6L-gZKPK2q-qxRUX3j",
      "authorship_tag": "ABX9TyPk4Sh1Ay118ZrEhOCvmu4J"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **`Necessary installations and library imports.`**"
      ],
      "metadata": {
        "id": "JIBzlEt01UHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install umap-learn"
      ],
      "metadata": {
        "id": "ZgAGi1lZhCKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install hdbscan"
      ],
      "metadata": {
        "id": "kNYAVo4Y4r88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "-ELDUgAw4t8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade openai"
      ],
      "metadata": {
        "id": "RcefdnrrdzTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim import corpora, models\n",
        "from sklearn.decomposition import PCA\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "from gensim.models import HdpModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "import plotly.graph_objects as go\n",
        "from collections import defaultdict\n",
        "import umap\n",
        "import hdbscan\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from sentence_transformers import SentenceTransformer,util\n",
        "import os\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "1seaIF3MZAh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`Dataset Preparation`**"
      ],
      "metadata": {
        "id": "axVQb9XB2u4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Read the original DataFrame\n",
        "old_df=pd.read_csv('your_path') #Replace with the actual path"
      ],
      "metadata": {
        "id": "G3rDulwBZAh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Extract the AI category\n",
        "new_df=old_df[old_df['categories'].str.contains('cs.AI')]\n",
        "new_df=new_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "hOtnHbpI_f_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Replacing the old year (date) column with year and month\n",
        "year=new_df['year'].str.slice(0,4).tolist()\n",
        "month=new_df['year'].str.slice(5,7).tolist()\n",
        "new_df.drop(columns=['year'])\n",
        "new_df['year']=[int(x) for x in year]\n",
        "new_df['month']=[int(x) for x in month]"
      ],
      "metadata": {
        "id": "bYqQRmie_f_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Extract papers from 2016 to 2023\n",
        "df=new_df[new_df['year']>=2016]\n",
        "df=df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "XW7ENcFj_f_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sort the DataFrame wrt the year\n",
        "df_sorted = df.sort_values(by='year',ascending=False)\n",
        "df_sorted=df_sorted.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "a8W8MHtD_f_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***`Enter the Query`***"
      ],
      "metadata": {
        "id": "628lAYuZ5Bet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query=input(\"Enter the query specifying the scientific field: \")"
      ],
      "metadata": {
        "id": "Lh0m54aX5F7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`Preprocessing step`**"
      ],
      "metadata": {
        "id": "DuJWTQdZ39pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define a function for preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def preprocess_text_updated(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and punctuation, and lemmatize words\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        # Remove punctuation\n",
        "        token = token.translate(str.maketrans('', '', string.punctuation))\n",
        "        # Apply lemmatization\n",
        "        token_lemm = lemmatizer.lemmatize(token)\n",
        "        # Remove stopwords and short words\n",
        "        if (token_lemm.lower() not in stop_words and token.lower() not in stop_words) and len(token_lemm)>=2:\n",
        "            filtered_tokens.append(token_lemm)\n",
        "\n",
        "    # Join the filtered tokens back into a sentence\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "\n",
        "    return preprocessed_text"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XZc4ADxQ4E22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Aplly the preprocessing function on the abstracts\n",
        "processed_abstracts=[preprocess_text_updated(abstract) for abstract in df_sorted['abstract'].tolist()]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IigkTnPv4g_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`The SBERT Embeddings step`**"
      ],
      "metadata": {
        "id": "ej1qJCAo4xOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define the SBERT model\n",
        "model = SentenceTransformer(\"allenai-specter\") # You can replace the model with any other model."
      ],
      "metadata": {
        "cellView": "form",
        "id": "_ArA7yxo48A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Apply the model on the processed abstracts and the query\n",
        "processed_abstracts_embeddings=model.encode(processed_abstracts)\n",
        "query_embeddings=model.encode(query)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mFtaN2aP5v0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`Relevant Abstracts Retrieval step`**"
      ],
      "metadata": {
        "id": "rI_n4O_67i3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Computing the Similarity between the Embeddings of the abstracts and the query\n",
        "similarity=np.array(util.cos_sim(query_embeddings,processed_abstracts_embeddings))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jaFjm64t7srP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Retrieve abstracts with similarity exceeding the threshold\n",
        "threshold=0.75  # Set the threshold you prefer\n",
        "selected_indices = np.where(similarity > threshold)[1]\n",
        "selected_processed_abstracts=[processed_abstracts[i] for i in selected_indices]\n",
        "selected_processed_abstracts_embeddings=[processed_abstracts_embeddings[i] for i in selected_indices]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Irs-rDu98AOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`Perform Clustering Step`**"
      ],
      "metadata": {
        "id": "2Sgj_DK18fIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Apply UMAP dimensionality reduction\n",
        "n_component=50  #The new dimesnionality value\n",
        "fit=umap.UMAP(n_neighbors=200,min_dist=0.0,n_components=n_component,metric='cosine')  # Select the paramaters you want\n",
        "selected_processed_abstracts_umap_embeddings=fit.fit_transform(selected_processed_abstracts_embeddings)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "W8KCuR008jDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Apply the clustering algorithm\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=130,min_samples=3,gen_min_span_tree=True) #Select the paramaters you want\n",
        "clusters=clusterer.fit(selected_processed_abstracts_umap_embeddings)\n",
        "cluster_labels=clusters.labels_\n",
        "\n",
        "# To view the clusters distribution\n",
        "unique_clusters, counts = np.unique(cluster_labels, return_counts=True)\n",
        "for cluster_id, count in zip(unique_clusters, counts):\n",
        "    if(cluster_id==-1):\n",
        "      print(\"Cluster:\", cluster_id, \" (Noise)\", \", Number of embeddings:\", count)\n",
        "      continue\n",
        "    print(\"Cluster:\", cluster_id, \", Number of embeddings:\", count)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4oYV3sjl9P12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`Topic Modeling Step`**"
      ],
      "metadata": {
        "id": "43I7SCtV-HSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Getting the optimum number of topics for each cluster\n",
        "num_of_topics=[]\n",
        "for cluster in range(len(unique_clusters)-1):\n",
        "  cluster_indices=np.array(np.where(cluster_labels==cluster))\n",
        "  abstracts_per_cluster=[selected_processed_abstracts[i] for i in cluster_indices[0]]\n",
        "  tokenized_abstracts_per_cluster=[a.split() for a in abstracts_per_cluster]\n",
        "  dictionary = corpora.Dictionary(tokenized_abstracts_per_cluster)\n",
        "  corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_abstracts_per_cluster]\n",
        "  topic_number_range=[5,10]\n",
        "  coherence_scores={}\n",
        "  number_of_trials_per_topic_number=20\n",
        "  for trial in range(number_of_trials_per_topic_number):\n",
        "    for num_top in topic_number_range:\n",
        "      lda_model = models.LdaModel(corpus, num_topics=num_top, id2word=dictionary,passes=10)\n",
        "      coherence_model = CoherenceModel(model=lda_model, texts=tokenized_abstracts_per_cluster, dictionary=dictionary, coherence='c_v')\n",
        "      coherence_score = coherence_model.get_coherence()\n",
        "      coherence_scores.setdefault(num_top, []).append(coherence_score)\n",
        "  average_coherence_scores = {num_topics: np.mean(scores) for num_topics, scores in coherence_scores.items()}\n",
        "  optimal_num_topics = max(average_coherence_scores, key=average_coherence_scores.get)\n",
        "  num_of_topics.append(optimal_num_topics)\n",
        "\n",
        "num_of_topics=np.array(num_of_topics)"
      ],
      "metadata": {
        "id": "97CjA2xa-QUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Apply LDA with the obtained optimum number of topics per each cluster\n",
        "lda_models=[]\n",
        "for cluster in range(len(unique_clusters)-1):\n",
        "  indices=np.array(np.where(cluster_labels==cluster))\n",
        "  cluster_indices=np.array(np.where(cluster_labels==cluster))\n",
        "  abstracts_per_cluster=[selected_processed_abstracts[i] for i in cluster_indices[0]]\n",
        "  tokenized_abstracts_per_cluster=[a.split() for a in abstracts_per_cluster]\n",
        "  dictionary = corpora.Dictionary(tokenized_abstracts_per_cluster)\n",
        "  corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_abstracts_per_cluster]\n",
        "  lda_model = models.LdaModel(corpus, num_topics=num_of_topics[cluster], id2word=dictionary,passes=10)\n",
        "  lda_models.append(lda_model)\n",
        "\n",
        "  #To view the topics distribution\n",
        "  print(\"For Cluster \",cluster,' :','\\n')\n",
        "  for topic_id, topic in lda_model.print_topics():\n",
        "      print(f\"Topic {topic_id}: {topic}\")\n",
        "      print('\\n')\n",
        "  print('------------------------------------------')"
      ],
      "metadata": {
        "id": "Zpxd1UQvCnYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`Topic Labeling step`**"
      ],
      "metadata": {
        "id": "y22m7Km-Es2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The labeling was done using Microsoft's Copilot Generative AI, and the labels were save in a csv file\n",
        "labeled_topics=pd.read_csv('your_path') # Replace with the actual path"
      ],
      "metadata": {
        "id": "Y-k8aJBkEzln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`Analyzing the topics trending bahavior`**"
      ],
      "metadata": {
        "id": "DVD3G9hIFWrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Calculating the the retrieved abstracts distribution across years and quarters\n",
        "filtered_df_query = df_sorted.iloc[selected_indices].copy()\n",
        "filtered_df_query=filtered_df_query.reset_index(drop=True)\n",
        "\n",
        "#Calculate per year quarter\n",
        "quarters_map = {\n",
        "    1: 'Q1', 2: 'Q1', 3: 'Q1',\n",
        "    4: 'Q2', 5: 'Q2', 6: 'Q2',\n",
        "    7: 'Q3', 8: 'Q3', 9: 'Q3',\n",
        "    10: 'Q4', 11: 'Q4', 12: 'Q4'\n",
        "}\n",
        "filtered_df_query['Quarter'] = filtered_df_query['month'].map(quarters_map)\n",
        "year_quarter_query_counts = filtered_df_query.groupby(['year', 'Quarter']).size().reset_index(name='Count')\n",
        "year_quarter_query_counts.sort_values(by=['year', 'Quarter'], inplace=True)\n",
        "\n",
        "#Calculate per year\n",
        "temp=year_quarter_query_counts.groupby('year')['Count'].sum()\n",
        "year_query_counts=pd.DataFrame(temp).reset_index()\n",
        "year_query_counts.columns = ['Year', 'Count']"
      ],
      "metadata": {
        "cellView": "form",
        "id": "C97pRdhSFdr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Help functions\n",
        "\n",
        "#-----------create an empty df with the desired year-quarter distribution---------------------------------\n",
        "def create_df():\n",
        "  years = []\n",
        "  quarters = []\n",
        "  counts = []\n",
        "\n",
        "  # Generate data for years from 2016 to 2023\n",
        "  for year in range(2016, 2024):\n",
        "    # Generate quarters for each year (4 quarters for all years except 2023)\n",
        "    if year < 2023:\n",
        "          for quarter in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
        "              years.append(year)\n",
        "              quarters.append(quarter)\n",
        "              counts.append(0)\n",
        "    else:\n",
        "          for quarter in ['Q1', 'Q2']:\n",
        "              years.append(year)\n",
        "              quarters.append(quarter)\n",
        "              counts.append(0)\n",
        "\n",
        "  # Create DataFrame\n",
        "  data = {'year': years, 'Quarter': quarters, 'Count': counts}\n",
        "  df = pd.DataFrame(data)\n",
        "  return df\n",
        "\n",
        "\n",
        "#-----------Function to retrieve the indix of an item in a list---------------------------------\n",
        "def indices_where_equal(val, list):\n",
        "    for i,el in enumerate(list):\n",
        "      if(el==val):\n",
        "        return i\n",
        "    return -1"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wMRmvw-gGUWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Method 1\n",
        "for cluster in range(len(unique_clusters)-1):\n",
        "  df_cluster=labeled_topics[labeled_topics['Cluster']==cluster]\n",
        "  df_cluster=df_cluster.reset_index(drop=True)\n",
        "  dominant_topic_document_indices = defaultdict(list)\n",
        "  cluster_indices=np.array(np.where(cluster_labels==cluster))\n",
        "  abstracts_per_cluster=[selected_processed_abstracts[i] for i in cluster_indices[0]]\n",
        "  tokenized_abstracts_per_cluster=[a.split() for a in abstracts_per_cluster]\n",
        "  dictionary = corpora.Dictionary(tokenized_abstracts_per_cluster)\n",
        "  for i,doc in enumerate(tokenized_abstracts_per_cluster):\n",
        "    # Convert the document to bag of words format\n",
        "    doc_bow = dictionary.doc2bow(doc)\n",
        "    # Get the topic distribution for the document\n",
        "    topic_distribution = lda_models[cluster].get_document_topics(doc_bow)\n",
        "    # Extract the dominant topic and increment the count for that topic\n",
        "    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
        "    dominant_topic_document_indices[dominant_topic].append(i)\n",
        "\n",
        "  fig = go.Figure()\n",
        "  for top_ind in range(len(df_cluster)):\n",
        "    merged_topics=df_cluster['Merged Topics'][top_ind].split(',')\n",
        "    merged_topics=[int(num) for num in merged_topics]\n",
        "    cumelative_df=create_df()\n",
        "\n",
        "    for ind,top in enumerate(merged_topics):\n",
        "      abstracts_per_topic=[abstracts_per_cluster[j] for j in dominant_topic_document_indices[top]]\n",
        "      abstracts_per_topic_indices_in_df=[indices_where_equal(abst, selected_processed_abstracts) for abst in abstracts_per_topic]\n",
        "      new_filtered_df = filtered_df_query.iloc[abstracts_per_topic_indices_in_df].copy()\n",
        "      quarter_counts = new_filtered_df.groupby(['year', 'Quarter']).size().reset_index(name='Count')\n",
        "      cumelative_df_indexed = cumelative_df.set_index(['year', 'Quarter'])\n",
        "      quarter_counts_indexed = quarter_counts.set_index(['year', 'Quarter'])\n",
        "      result = cumelative_df_indexed.add(quarter_counts_indexed, fill_value=0)\n",
        "      result['Count']=result['Count'].astype(int)\n",
        "      result.reset_index(inplace=True)\n",
        "      cumelative_df=result.copy()\n",
        "\n",
        "    temp=cumelative_df.groupby('year')['Count'].sum()\n",
        "    cumelative_df=pd.DataFrame(temp).reset_index()\n",
        "    cumelative_df.columns = ['year', 'Count']\n",
        "    label=\"Topic \"+str(top_ind)+\": \"+df_cluster['Topic Label'][top_ind]\n",
        "    fig.add_trace(go.Scatter(x=cumelative_df['year'].astype(str),\n",
        "                              y=cumelative_df['Count'],\n",
        "                              mode='lines',\n",
        "                              name=label,\n",
        "                              visible='legendonly'  # Set visibility to 'legendonly' by default\n",
        "                              ))\n",
        "\n",
        "  fig.update_layout(\n",
        "      title='Papers per year for Cluster '+str(cluster),\n",
        "      xaxis_title='Year',\n",
        "      yaxis_title='Paper Count',\n",
        "      xaxis=dict(tickangle=-45),\n",
        "      legend=dict(traceorder='normal')\n",
        "  )\n",
        "\n",
        "  fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Do_zIeHLHLpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Method 2\n",
        "for cluster in range(len(unique_clusters)-1):\n",
        "  df_cluster=labeled_topics[labeled_topics['Cluster']==cluster]\n",
        "  df_cluster=df_cluster.reset_index(drop=True)\n",
        "  dominant_topic_document_indices = defaultdict(list)\n",
        "  cluster_indices=np.array(np.where(cluster_labels==cluster))\n",
        "  abstracts_per_cluster=[selected_processed_abstracts[i] for i in cluster_indices[0]]\n",
        "  tokenized_abstracts_per_cluster=[a.split() for a in abstracts_per_cluster]\n",
        "  dictionary = corpora.Dictionary(tokenized_abstracts_per_cluster)\n",
        "  for i,doc in enumerate(tokenized_abstracts_per_cluster):\n",
        "    # Convert the document to bag of words format\n",
        "    doc_bow = dictionary.doc2bow(doc)\n",
        "    # Get the topic distribution for the document\n",
        "    topic_distribution = lda_models[cluster].get_document_topics(doc_bow)\n",
        "    # Extract the dominant topic and increment the count for that topic\n",
        "    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
        "    dominant_topic_document_indices[dominant_topic].append(i)\n",
        "\n",
        "  fig = go.Figure()\n",
        "  for top_ind in range(len(df_cluster)):\n",
        "    merged_topics=df_cluster['Merged Topics'][top_ind].split(',')\n",
        "    merged_topics=[int(num) for num in merged_topics]\n",
        "    cumelative_df=create_df()\n",
        "\n",
        "    for ind,top in enumerate(merged_topics):\n",
        "      abstracts_per_topic=[abstracts_per_cluster[j] for j in dominant_topic_document_indices[top]]\n",
        "      abstracts_per_topic_indices_in_df=[indices_where_equal(abst, selected_processed_abstracts) for abst in abstracts_per_topic]\n",
        "      new_filtered_df = filtered_df_query.iloc[abstracts_per_topic_indices_in_df].copy()\n",
        "      quarter_counts = new_filtered_df.groupby(['year', 'Quarter']).size().reset_index(name='Count')\n",
        "      cumelative_df_indexed = cumelative_df.set_index(['year', 'Quarter'])\n",
        "      quarter_counts_indexed = quarter_counts.set_index(['year', 'Quarter'])\n",
        "      result = cumelative_df_indexed.add(quarter_counts_indexed, fill_value=0)\n",
        "      result['Count']=result['Count'].astype(int)\n",
        "      result.reset_index(inplace=True)\n",
        "      cumelative_df=result.copy()\n",
        "\n",
        "    temp=cumelative_df.groupby('year')['Count'].sum()\n",
        "    cumelative_df=pd.DataFrame(temp).reset_index()\n",
        "    cumelative_df.columns = ['year', 'Count']\n",
        "    cumelative_df['Normalized_Count'] = cumelative_df['Count'] / year_query_counts['Count']\n",
        "    label=\"Topic \"+str(top_ind)+\": \"+df_cluster['Topic Label'][top_ind]\n",
        "    fig.add_trace(go.Scatter(x=cumelative_df['year'].astype(str),\n",
        "                              y=cumelative_df['Normalized_Count'],\n",
        "                              mode='lines',\n",
        "                              name=label,\n",
        "                              visible='legendonly'  # Set visibility to 'legendonly' by default\n",
        "                              ))\n",
        "\n",
        "  fig.update_layout(\n",
        "      title='Topic papers to Query papers per year ratio for Cluster  '+str(cluster),\n",
        "      xaxis_title='Year',\n",
        "      yaxis_title='Ratio',\n",
        "      xaxis=dict(tickangle=-45),\n",
        "      legend=dict(traceorder='normal')\n",
        "  )\n",
        "\n",
        "  fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XqB0p0j1MnJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Method 3\n",
        "for cluster in range(len(unique_clusters)-1):\n",
        "  df_cluster=labeled_topics[labeled_topics['Cluster']==cluster]\n",
        "  df_cluster=df_cluster.reset_index(drop=True)\n",
        "  dominant_topic_document_indices = defaultdict(list)\n",
        "  cluster_indices=np.array(np.where(cluster_labels==cluster))\n",
        "  abstracts_per_cluster=[selected_processed_abstracts[i] for i in cluster_indices[0]]\n",
        "  tokenized_abstracts_per_cluster=[a.split() for a in abstracts_per_cluster]\n",
        "  dictionary = corpora.Dictionary(tokenized_abstracts_per_cluster)\n",
        "  for i,doc in enumerate(tokenized_abstracts_per_cluster):\n",
        "    # Convert the document to bag of words format\n",
        "    doc_bow = dictionary.doc2bow(doc)\n",
        "    # Get the topic distribution for the document\n",
        "    topic_distribution = lda_models[cluster].get_document_topics(doc_bow)\n",
        "    # Extract the dominant topic and increment the count for that topic\n",
        "    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
        "    dominant_topic_document_indices[dominant_topic].append(i)\n",
        "\n",
        "  fig = go.Figure()\n",
        "  for top_ind in range(len(df_cluster)):\n",
        "    merged_topics=df_cluster['Merged Topics'][top_ind].split(',')\n",
        "    merged_topics=[int(num) for num in merged_topics]\n",
        "    cumelative_df=create_df()\n",
        "\n",
        "    for ind,top in enumerate(merged_topics):\n",
        "      abstracts_per_topic=[abstracts_per_cluster[j] for j in dominant_topic_document_indices[top]]\n",
        "      abstracts_per_topic_indices_in_df=[indices_where_equal(abst, selected_processed_abstracts) for abst in abstracts_per_topic]\n",
        "      new_filtered_df = filtered_df_query.iloc[abstracts_per_topic_indices_in_df].copy()\n",
        "      quarter_counts = new_filtered_df.groupby(['year', 'Quarter']).size().reset_index(name='Count')\n",
        "      cumelative_df_indexed = cumelative_df.set_index(['year', 'Quarter'])\n",
        "      quarter_counts_indexed = quarter_counts.set_index(['year', 'Quarter'])\n",
        "      result = cumelative_df_indexed.add(quarter_counts_indexed, fill_value=0)\n",
        "      result['Count']=result['Count'].astype(int)\n",
        "      result.reset_index(inplace=True)\n",
        "      cumelative_df=result.copy()\n",
        "\n",
        "    temp=cumelative_df.groupby('year')['Count'].sum()\n",
        "    cumelative_df=pd.DataFrame(temp).reset_index()\n",
        "    cumelative_df.columns = ['year', 'Count']\n",
        "    cumelative_df['Normalized_Count'] = cumelative_df['Count'] / year_query_counts['Count']\n",
        "    window_size = 5  # Set the value you want\n",
        "    cumelative_df['Moving_Average'] = cumelative_df['Normalized_Count'].rolling(window=window_size, min_periods=1).mean()\n",
        "    label=\"Topic \"+str(top_ind)+\": \"+df_cluster['Topic Label'][top_ind]\n",
        "    fig.add_trace(go.Scatter(x=cumelative_df['year'].astype(str),\n",
        "                              y=cumelative_df['Moving_Average'],\n",
        "                              mode='lines',\n",
        "                              name=label,\n",
        "                              visible='legendonly'  # Set visibility to 'legendonly' by default\n",
        "                              ))\n",
        "\n",
        "  fig.update_layout(\n",
        "      title='Topic papers to Query papers per year ratio using moving average of '+str(window_size) +' for Cluster '  +str(cluster),\n",
        "      xaxis_title='Year',\n",
        "      yaxis_title='the moving average ratio',\n",
        "      xaxis=dict(tickangle=-45),\n",
        "      legend=dict(traceorder='normal')\n",
        "  )\n",
        "\n",
        "  fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oepFcFGgNKuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Method 4\n",
        "for cluster in range(len(unique_clusters)-1):\n",
        "  df_cluster=labeled_topics[labeled_topics['Cluster']==cluster]\n",
        "  df_cluster=df_cluster.reset_index(drop=True)\n",
        "  dominant_topic_document_indices = defaultdict(list)\n",
        "  cluster_indices=np.array(np.where(cluster_labels==cluster))\n",
        "  abstracts_per_cluster=[selected_processed_abstracts[i] for i in cluster_indices[0]]\n",
        "  tokenized_abstracts_per_cluster=[a.split() for a in abstracts_per_cluster]\n",
        "  dictionary = corpora.Dictionary(tokenized_abstracts_per_cluster)\n",
        "  for i,doc in enumerate(tokenized_abstracts_per_cluster):\n",
        "    # Convert the document to bag of words format\n",
        "    doc_bow = dictionary.doc2bow(doc)\n",
        "    # Get the topic distribution for the document\n",
        "    topic_distribution = lda_models[cluster].get_document_topics(doc_bow)\n",
        "    # Extract the dominant topic and increment the count for that topic\n",
        "    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
        "    dominant_topic_document_indices[dominant_topic].append(i)\n",
        "\n",
        "  fig = go.Figure()\n",
        "  for top_ind in range(len(df_cluster)):\n",
        "    merged_topics=df_cluster['Merged Topics'][top_ind].split(',')\n",
        "    merged_topics=[int(num) for num in merged_topics]\n",
        "    cumelative_df=create_df()\n",
        "\n",
        "    for ind,top in enumerate(merged_topics):\n",
        "      abstracts_per_topic=[abstracts_per_cluster[j] for j in dominant_topic_document_indices[top]]\n",
        "      abstracts_per_topic_indices_in_df=[indices_where_equal(abst, selected_processed_abstracts) for abst in abstracts_per_topic]\n",
        "      new_filtered_df = filtered_df_query.iloc[abstracts_per_topic_indices_in_df].copy()\n",
        "      quarter_counts = new_filtered_df.groupby(['year', 'Quarter']).size().reset_index(name='Count')\n",
        "      cumelative_df_indexed = cumelative_df.set_index(['year', 'Quarter'])\n",
        "      quarter_counts_indexed = quarter_counts.set_index(['year', 'Quarter'])\n",
        "      result = cumelative_df_indexed.add(quarter_counts_indexed, fill_value=0)\n",
        "      result['Count']=result['Count'].astype(int)\n",
        "      result.reset_index(inplace=True)\n",
        "      cumelative_df=result.copy()\n",
        "\n",
        "    cumelative_df['Normalized_Count'] = cumelative_df['Count'] / year_quarter_query_counts['Count']\n",
        "    window_size = 20  # Set the value you want\n",
        "    cumelative_df['Moving_Average'] = cumelative_df['Normalized_Count'].rolling(window=window_size, min_periods=1).mean()\n",
        "    label=\"Topic \"+str(top_ind)+\": \"+df_cluster['Topic Label'][top_ind]\n",
        "    fig.add_trace(go.Scatter(x=cumelative_df['year'].astype(str)+ '-' + cumelative_df['Quarter'],\n",
        "                              y=cumelative_df['Moving_Average'],\n",
        "                              mode='lines',\n",
        "                              name=label,\n",
        "                              visible='legendonly'  # Set visibility to 'legendonly' by default\n",
        "                              ))\n",
        "\n",
        "  fig.update_layout(\n",
        "      title='Topic papers to Query papers per year ratio using moving average of '+str(window_size) +' for Cluster '  +str(cluster),\n",
        "      xaxis_title='Year-Quarter',\n",
        "      yaxis_title='the moving average ratio',\n",
        "      xaxis=dict(tickangle=-45),\n",
        "      legend=dict(traceorder='normal')\n",
        "  )\n",
        "\n",
        "  fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qPHWpPwtNyDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`Summarization Step`**"
      ],
      "metadata": {
        "id": "wOsYGGEoPJhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Help Functions\n",
        "def generate_summary(prompt,key):\n",
        "  client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=key,\n",
        ")\n",
        "  chat_completion = client.chat.completions.create(\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt,\n",
        "          }\n",
        "      ],\n",
        "      model=\"gpt-3.5-turbo-0125\",\n",
        "  )\n",
        "  return chat_completion.choices[0].message.content\n",
        "\n",
        "def get_topic(abstracts_per_cluster,abstracts_per_cluster_emb,topic_num,cluster,lda_models):\n",
        "  abstracts_per_topic=[]\n",
        "  abstracts_emb_per_topic=[]\n",
        "  tokenized_abstracts_per_cluster = [a.split() for a in abstracts_per_cluster]\n",
        "  dictionary = corpora.Dictionary(tokenized_abstracts_per_cluster)\n",
        "  for i,doc in enumerate(tokenized_abstracts_per_cluster):\n",
        "      # Convert the document to bag of words format\n",
        "      doc_bow = dictionary.doc2bow(doc)\n",
        "      # Get the topic distribution for the document\n",
        "      topic_distribution = lda_models[cluster].get_document_topics(doc_bow)\n",
        "      # Extract the dominant topic and increment the count for that topic\n",
        "      dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
        "      if(dominant_topic==topic_num):\n",
        "        abstracts_per_topic.append(abstracts_per_cluster[i])\n",
        "        abstracts_emb_per_topic.append(abstracts_per_cluster_emb[i])\n",
        "  return abstracts_per_topic, abstracts_emb_per_topic\n",
        "\n",
        "def find_indices(x, y):\n",
        "    # Initialize a list to store indices\n",
        "    indices_list = []\n",
        "\n",
        "    # Create a dictionary to map elements to their indices in x\n",
        "    index_map = {value: index for index, value in enumerate(x)}\n",
        "\n",
        "    # Iterate over each element in y\n",
        "    for elem in y:\n",
        "        # Find the index of elem in x using the dictionary\n",
        "        index = index_map.get(elem)\n",
        "        # If index is not None, append it to the indices_list\n",
        "        if index is not None:\n",
        "            indices_list.append(index)\n",
        "        # If index is None, append None to indicate that elem is not found in x\n",
        "        else:\n",
        "            indices_list.append(None)\n",
        "\n",
        "    return indices_list\n",
        "def generate_summary_prompt(topic_label, abstracts):\n",
        "    \"\"\"\n",
        "    Generate a prompt for summarizing abstracts dominated by a specific topic.\n",
        "\n",
        "    Args:\n",
        "    - topic_label (str): The label of the topic.\n",
        "    - abstracts (list of str): A list of abstracts dominated by the specified topic.\n",
        "\n",
        "    Returns:\n",
        "    - prompt (str): The generated prompt.\n",
        "    \"\"\"\n",
        "    # Construct the prompt header with the topic label\n",
        "    prompt = f\"Topic Label: {topic_label}\\n\\n\"\n",
        "\n",
        "    # Add each abstract to the prompt under the \"Abstracts Dominated by this Topic\" section\n",
        "    prompt += \"Abstracts Dominated by this Topic:\\n\"\n",
        "    for i, abstract in enumerate(abstracts, start=1):\n",
        "        prompt += f\"{i}-{abstract}\\n\"\n",
        "\n",
        "    # Add the summary section with a placeholder\n",
        "    prompt += \"\\nSummary:\\n\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def select_top_abstracts(topic_label_embedding, abstract_embeddings, abstracts, top_n):\n",
        "    \"\"\"\n",
        "    Select the top N abstracts most similar to the topic label embedding.\n",
        "\n",
        "    Args:\n",
        "    - topic_label_embedding (np.array): SBERT embedding of the topic label.\n",
        "    - abstract_embeddings (list of np.array): List of SBERT embeddings of abstracts.\n",
        "    - abstracts (list of str): List of abstracts.\n",
        "    - top_n (int): Number of top abstracts to select.\n",
        "\n",
        "    Returns:\n",
        "    - top_abstracts (list of str): List of top N abstracts most similar to the topic label.\n",
        "    \"\"\"\n",
        "    # Calculate cosine similarity between the topic label embedding and abstract embeddings\n",
        "    similarities = np.array(util.cos_sim(topic_label_embedding, abstract_embeddings))[0]\n",
        "    # Sort abstracts based on similarity scores in descending order\n",
        "    sorted_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "    # Select the top N abstracts\n",
        "    top_abstracts = [abstracts[i] for i in sorted_indices[:top_n]]\n",
        "\n",
        "    return top_abstracts"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aUO1hEa-PPMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title generate the summary\n",
        "topic_label=input(\"Enter the topic you want to summarize: \")\n",
        "topic_info=labeled_topics[labeled_topics[\"Topic Label\"]==topic_label].reset_index(drop=True)\n",
        "cluster=topic_info[\"Cluster\"][0]\n",
        "merged_topics=topic_info['Merged Topics'][0].split(',')\n",
        "merged_topics_list=[int(num) for num in merged_topics]\n",
        "topic_label_emb=model.encode(topic_label)\n",
        "cluster_indices=np.array(np.where(cluster_labels==cluster))\n",
        "abstracts=[selected_processed_abstracts[i] for i in cluster_indices[0]]\n",
        "abstracts_emb=[selected_processed_abstracts_embeddings[i] for i in cluster_indices[0]]\n",
        "processed_abstracts_per_topic=[]\n",
        "processed_abstracts_emb_per_topic=[]\n",
        "\n",
        "for top_ind in range(len(merged_topics_list)):\n",
        "  abs_list,abs_emb_list=get_topic(abstracts, abstracts_emb,merged_topics_list[top_ind],cluster,lda_models)\n",
        "  processed_abstracts_per_topic=processed_abstracts_per_topic+abs_list\n",
        "  processed_abstracts_emb_per_topic=processed_abstracts_emb_per_topic+abs_emb_list\n",
        "\n",
        "top_relevant_abstracts_for_topic=select_top_abstracts(topic_label_emb,np.array(processed_abstracts_emb_per_topic), processed_abstracts_per_topic,50)\n",
        "temp=find_indices(processed_abstracts,top_relevant_abstracts_for_topic)\n",
        "abstracts_per_topic=[df_sorted['abstract'][i] for i in temp]\n",
        "prompt=generate_summary_prompt(topic_label,abstracts_per_topic)\n",
        "\n",
        "key=os.getenv(\"YOUR_API_KEY\") # Replace it with the name of your env file\n",
        "summary=generate_summary(prompt,key)\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "4pRAmOI7P1yE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}